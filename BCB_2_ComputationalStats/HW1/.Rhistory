version
library(vegan)
install.packages("vegan")
install.packages('vegan')
T <- readTrees("data/primates_tree.nex")[1]
RB
rb
q()
library(rb)
mytree<- pbtree(n=900)
plot(mytree)
t1<-fastBM(tree=mytree)
t2<-fastBM(tree=mytree)
cor(t1,2)
library(phytools)
library(ape)
library(geiger)
mytree<- pbtree(n=900)
plot(mytree)
t1<-fastBM(tree=mytree)
t2<-fastBM(tree=mytree)
cor(t1,2)
cor(t1,t2)
t1.pic<-lapply(1:nsim, function(j) pic(x = t1[[j]],phy = mytree[[j]]))
t2.pic<-lapply(1:nsim, function(j) pic(x = t2[[j]],phy = mytree[[j]]))
cor.pic<-unlist(lapply(1:nsim, function(j) cor(t1.pic[[j]],t2.pic[[j]])))
hist(cor.pic, xlim=c(-1,1))
Hyd.dat<-read.csv("TutorialData/HydromantesMns.csv", header=TRUE, row.names = 1)
setwd("~/Documents/Classes/MacroEvolution/tutorial")
setwd("~/Documents/Classes/MacroEvolution/tutorial")
Hyd.dat<-read.csv("TutorialData/HydromantesMns.csv", header=TRUE, row.names = 1)
Hyd.dat  #Notice we read in the first row as row.names. These MUST match the names in the phylogeny
#Now, prune the tree to match the data and vice-versa:
Hydro.new<-treedata(phy=ManderTree,data = Hyd.dat, warnings=FALSE)
#Here is a large time-dated molecular phylogeny (a chronogram):
ManderTree<-read.tree("TutorialData/Mander.tre",tree.names=T)
#Now, prune the tree to match the data and vice-versa:
Hydro.new<-treedata(phy=ManderTree,data = Hyd.dat, warnings=FALSE)
plot(Hydro.new$phy)  #We have matched data to the tree!
plot(fake.t1,fake.t2)
fakeTree=pbtree(n=900)
plot(fakeTree)
fake.t1=fastBM(tree=fakeTree)
fake.t2=fastBM(tree=fakeTree)
cor(fake.t1,fake.t2)
plot(fake.t1,fake.t2)
?fastBM
fake.t1=fastBM(tree=fakeTree,mu=2)
fake.t2=fastBM(tree=fakeTree,mu=5)
cor(fake.t1,fake.t2)
plot(fake.t1,fake.t2)
fake.t2=fastBM(tree=fakeTree,mu=10)
cor(fake.t1,fake.t2)
plot(fake.t1,fake.t2)
fake.t2=fastBM(tree=fakeTree,mu=5)
cor(fake.t1,fake.t2)
plot(fake.t1,fake.t2)
#pring isn't need but we do need to match the data to the tree
Hydro.new<-treedata(phy=fakeTree,data = fake, warnings=FALSE)
cor.test(fake.t1,fake.t2)
anova(lm(HL.new~SVL.new))
anova(lm(fake.t1~fake.t2))
X.pic<-pic(x = fake.t1, phy = fakeTree)
Y.pic<-pic(x = fake.t2, phy = fakeTree)
cor.test(X.pic,Y.pic)
plot(X.pic,Y.pic)
anova(lm(Y.pic~X.pic+0))   #regression through the origin (b/c order of taxa for contrast is arbitrary [can 'spin' on node])
summary(lm(Y.pic~X.pic+0))
setwd("~/")
str_pad(x, 1, pad = "0")
library(stringr)
str_pad(x, 1, pad = "0")
str_pad(1, 1, pad = "0")
str_pad(1, 1, pad = "00")
formatC(4, width = 2, format = "d", flag = "0")
formatC(11, width = 2, format = "d", flag = "0")
library(dplyr, quietly = T, warn.conflicts = F)
install.packages(dplyr)
install.packages('dplyr')
library(dplyr, quietly = T, warn.conflicts = F)
psi <- seq(from = 1e-6, to = 1-1e-6, length.out = 1000)
psi
plot(cars)
setwd("~/Documents/Classes/BCB_2_ComputationalStats/HW1")
library(dplyr, quietly = T, warn.conflicts = F)
# L(D=d_i | G=g)
log.likelihood.d.g <- function(g, a, m, e, k){
-k*log(m) +
sum(
log(ifelse(a == 1,
(m - g) * e/3 + g * (1 - e),
(m - g) * (1 - e) + g * e/3
)
))
}
log.likelihood.G <- function(d, m=2){
do.call(rbind,
lapply(split(d, factor(d$i)), function(d_i){
sapply(0:m, log.likelihood.d.g, a=d_i$a,
m=m, e=d_i$e, k=nrow(d_i))
})
)
}
logsum <- function(x){
# adapted from http://andrewgelman.com/2016/06/11/log-sum-of-exponentials/
logsum2 <- function(a,b){
max(a,b) + log(exp(a - max(a,b)) + exp(b - max(a,b)))
}
Reduce(logsum2, x[-1], x[1])
}
logcolsums <- function(m){
apply(m, 2, logsum)
}
log.likelihood.psi <- function(psi, d, m){
# L(G=g_i|d_j)f(g_i|m,psi) -- (n_i X n_j matrix)
A <- t(log.likelihood.G(d)) + dbinom(0:m, m, psi, log=TRUE)
# Now I need to *sum* the log probabilities. This can lead
# to numeric problems ('Inf' values). It does not cause
# problems with our particular dataset, though.
# sumA <- log(colSums(exp(A)))
# to perform this computation safely:
sumA <- logcolsums(A)
# Multiply by negative -1 since optim searches for minima
-1 * sum(sumA)
}
d= read_table("data/Pos962_data.txt",header=T)
d= read.table("data/Pos962_data.txt",header=T)
head(d)
d$e <- 10^(-d$q/10)
require(ggplot2, quietly = T, warn.conflicts = F)
psi <- seq(from = 1e-6, to = 1-1e-6, length.out = 1000)
ll <- do.call(rbind, lapply(psi, log.likelihood.psi, d = d, m = 2))
ll.plot <- data.frame(psi = psi, ll = ll)
ggplot(ll.plot, aes(x = psi, y = -ll)) + geom_line() +labs(y = "log likelihood", x = expression(psi))
estimate.psi <- function(d){optim(0.5, log.likelihood.psi(), d=d, m=2, method='Brent', lower=0, upper=1)$par}
mle <- estimate.psi(d)
estimate.psi <- function(d)
{
optim(0.5, log.likelihood.psi(d), d=d, m=2, method='Brent', lower=0, upper=1)$par
}
mle <- estimate.psi(d)
mle <- estimate.psi(d)
log.likelihood.G <- function(d=d, m=2){
do.call(rbind,
lapply(split(d, factor(d$i)), function(d_i){
sapply(0:m, log.likelihood.d.g, a=d_i$a,
m=m, e=d_i$e, k=nrow(d_i))
})
)
}
mle <- estimate.psi(d)
log.likelihood.psi <- function(psi, d, m){
# L(G=g_i|d_j)f(g_i|m,psi) -- (n_i X n_j matrix)
print "Here"
A <- t(log.likelihood.G(d)) + dbinom(0:m, m, psi, log=TRUE)
print "TEst"
# Now I need to *sum* the log probabilities. This can lead
# to numeric problems ('Inf' values). It does not cause
# problems with our particular dataset, though.
# sumA <- log(colSums(exp(A)))
# to perform this computation safely:
sumA <- logcolsums(A)
# Multiply by negative -1 since optim searches for minima
-1 * sum(sumA)
}
log.likelihood.psi <- function(psi, d, m){
# L(G=g_i|d_j)f(g_i|m,psi) -- (n_i X n_j matrix)
A <- t(log.likelihood.G(d)) + dbinom(0:m, m, psi, log=TRUE)
# Now I need to *sum* the log probabilities. This can lead
# to numeric problems ('Inf' values). It does not cause
# problems with our particular dataset, though.
# sumA <- log(colSums(exp(A)))
# to perform this computation safely:
sumA <- logcolsums(A)
# Multiply by negative -1 since optim searches for minima
-1 * sum(sumA)
}
log.likelihood.psi <- function(psi, d, m){
# L(G=g_i|d_j)f(g_i|m,psi) -- (n_i X n_j matrix)
print ("TEst")
A <- t(log.likelihood.G(d)) + dbinom(0:m, m, psi, log=TRUE)
# Now I need to *sum* the log probabilities. This can lead
# to numeric problems ('Inf' values). It does not cause
# problems with our particular dataset, though.
# sumA <- log(colSums(exp(A)))
# to perform this computation safely:
sumA <- logcolsums(A)
# Multiply by negative -1 since optim searches for minima
-1 * sum(sumA)
}
print ("here")
log.likelihood.psi <- function(psi, d, m){
# L(G=g_i|d_j)f(g_i|m,psi) -- (n_i X n_j matrix)
print ("TEst")
A <- t(log.likelihood.G(d)) + dbinom(0:m, m, psi, log=TRUE)
print ("here")
# Now I need to *sum* the log probabilities. This can lead
# to numeric problems ('Inf' values). It does not cause
# problems with our particular dataset, though.
# sumA <- log(colSums(exp(A)))
# to perform this computation safely:
sumA <- logcolsums(A)
# Multiply by negative -1 since optim searches for minima
-1 * sum(sumA)
}
mle <- estimate.psi(d)
log.likelihood.psi <- function(psi, d, m){
# L(G=g_i|d_j)f(g_i|m,psi) -- (n_i X n_j matrix)
print ("TEst")
A <- t(log.likelihood.G(d,m)) + dbinom(0:m, m, psi, log=TRUE)
print ("here")
# Now I need to *sum* the log probabilities. This can lead
# to numeric problems ('Inf' values). It does not cause
# problems with our particular dataset, though.
# sumA <- log(colSums(exp(A)))
# to perform this computation safely:
sumA <- logcolsums(A)
# Multiply by negative -1 since optim searches for minima
-1 * sum(sumA)
}
mle <- estimate.psi(d)
split(d,factor(d$i))
log.likelihood.G <- function(d, m=2){
do.call(rbind,
lapply(split(d, factor(d$i)), function(d_i){
sapply(0:m, log.likelihood.d.g, a=d_i$a, m=m, e=d_i$e, k=nrow(d_i))
})
)
}
log.likelihood.G <- function(d, m=2){
do.call(rbind,
lapply(split(d, factor(d$i)), function(d_i){
sapply(0:m, log.likelihood.d.g, a=d_i$a, m=m, e=d_i$e, k=nrow(d_i))
})
)
}
split(d,factor(d$i))
mle <- estimate.psi(d)
log.likelihood.d.g <- function(g, a, m, e, k){
print("d.g.")
-k*log(m) +
sum(
log(ifelse(a == 1,
(m - g) * e/3 + g * (1 - e),
(m - g) * (1 - e) + g * e/3
)
))
}
log.likelihood.G <- function(d, m=2){
print("Like.G")
do.call(rbind,
lapply(split(d, factor(d$i)), function(d_i){
sapply(0:m, log.likelihood.d.g, a=d_i$a, m=m, e=d_i$e, k=nrow(d_i))
})
)
}
mle <- estimate.psi(d)
log.likelihood.G <- function(d, m=2){
print("Like.G")
splitData = split(d, factor(d$i))
do.call(rbind,
lapply(splitData, function(d_i){
sapply(0:m, log.likelihood.d.g, a=d_i$a, m=m, e=d_i$e, k=nrow(d_i))
})
)
}
mle <- estimate.psi(d)
log.likelihood.G <- function(d, m=2){
print("Like.G")
splitData = split(d, factor(d$i))
print("SPlit")
do.call(rbind,
lapply(splitData, function(d_i){
sapply(0:m, log.likelihood.d.g, a=d_i$a, m=m, e=d_i$e, k=nrow(d_i))
})
)
}
mle <- estimate.psi(d)
log.likelihood.G <- function(d, m=2){
print("Like.G")
print(head(d))
splitData = split(d, factor(d$i))
print("SPlit")
do.call(rbind,
lapply(splitData, function(d_i){
sapply(0:m, log.likelihood.d.g, a=d_i$a, m=m, e=d_i$e, k=nrow(d_i))
})
)
}
mle <- estimate.psi(d)
log.likelihood.G <- function(d, m=2){
print("Like.G")
print("TEST")
print(d)
splitData = split(d, factor(d$i))
print("SPlit")
do.call(rbind,
lapply(splitData, function(d_i){
sapply(0:m, log.likelihood.d.g, a=d_i$a, m=m, e=d_i$e, k=nrow(d_i))
})
)
}
mle <- estimate.psi(d)
log.likelihood.psi <- function(psi, d, m){
# L(G=g_i|d_j)f(g_i|m,psi) -- (n_i X n_j matrix)
print (head(d))
A <- t(log.likelihood.G(d,m)) + dbinom(0:m, m, psi, log=TRUE)
print ("here")
# Now I need to *sum* the log probabilities. This can lead
# to numeric problems ('Inf' values). It does not cause
# problems with our particular dataset, though.
# sumA <- log(colSums(exp(A)))
# to perform this computation safely:
sumA <- logcolsums(A)
# Multiply by negative -1 since optim searches for minima
-1 * sum(sumA)
}
mle <- estimate.psi(d)
estimate.psi <- function(d)
{
optim(0.5, log.likelihood.psi, d=d, m=2, method='Brent', lower=0, upper=1)$par
}
mle <- estimate.psi(d)
library(dplyr, quietly = T, warn.conflicts = F)
# L(D=d_i | G=g)
log.likelihood.d.g <- function(g, a, m, e, k){
print("d.g.")
-k*log(m) +
sum(
log(ifelse(a == 1,
(m - g) * e/3 + g * (1 - e),
(m - g) * (1 - e) + g * e/3
)
))
}
# Returns a matrix individuals as rows and genotypes as columns
log.likelihood.G <- function(d, m=2){
do.call(rbind,
lapply(split(d, factor(d$i)), function(d_i){
sapply(0:m, log.likelihood.d.g, a=d_i$a, m=m, e=d_i$e, k=nrow(d_i))
})
)
}
log.likelihood.psi <- function(psi, d, m){
# L(G=g_i|d_j)f(g_i|m,psi) -- (n_i X n_j matrix)
A <- t(log.likelihood.G(d,m)) + dbinom(0:m, m, psi, log=TRUE)
# Now I need to *sum* the log probabilities. This can lead
# to numeric problems ('Inf' values). It does not cause
# problems with our particular dataset, though.
# sumA <- log(colSums(exp(A)))
# to perform this computation safely:
sumA <- logcolsums(A)
# Multiply by negative -1 since optim searches for minima
-1 * sum(sumA)
}
d = read.table("data/Pos962_data.txt",header=T)
d$e <- 10^(-d$q/10) # compute error probability once
estimate.psi <- function(d)
{
optim(0.5, log.likelihood.psi, d=d, m=2, method='Brent', lower=0, upper=1)$par
}
mle <- estimate.psi(d)
mle
mle
d = read.table("data/Pos964_data.txt",header=T)
mle <- estimate.psi(d)
mle
